{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEcecokKMM/C88PMTp5fyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilm52626/Caba_nlp/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-okGjN-D6sTC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg_pIbnq7Xen"
      },
      "source": [
        "NLP, 텍스트 분석\r\n",
        "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\r\n",
        "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\r\n",
        "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\r\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\r\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\r\n",
        "    - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\r\n",
        "    - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\r\n",
        "    - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\r\n",
        "    - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsgeF_F3Fmik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TYOPYv9Fmqn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCI2ySnpFnN2"
      },
      "source": [
        "## Text Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FX7Fx2aC97-",
        "outputId": "7c98b5ab-c109-47bb-8272-fe8fa3d84ca6"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt') #nltk는 영어!\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfSNavFS7LBZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXSiYc23Di3T",
        "outputId": "f92381cf-95d3-456c-b454-608ce02cdf2a"
      },
      "source": [
        "# 문장 토큰화(sent tokenize): 마침표, 개행문자(\\n), 정규표현식\r\n",
        "\r\n",
        "from nltk import sent_tokenize\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\r\n",
        "               You can see it out your window or on your television. \\\r\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\r\n",
        "sentences = sent_tokenize(text=text_sample)\r\n",
        "print(type(sentences),len(sentences))\r\n",
        "print(sentences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koBMa5XHD_5s",
        "outputId": "9b625e05-c6b5-4087-989d-831a50e2cbcc"
      },
      "source": [
        "# 단어 토큰화(word_tokenize): 공백, comma, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\r\n",
        "words = word_tokenize(sentence)\r\n",
        "print(type(words), len(words))\r\n",
        "print(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJa0Uey_EE1z",
        "outputId": "a2d12c72-1f56-4b04-855d-14b4571f91a1"
      },
      "source": [
        "# 한번에 하는 방법\r\n",
        "from nltk import word_tokenize, sent_tokenize\r\n",
        "\r\n",
        "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\r\n",
        "def tokenize_text(text):\r\n",
        "    \r\n",
        "    # 문장별로 분리 토큰\r\n",
        "    sentences = sent_tokenize(text)\r\n",
        "    # 분리된 문장별 단어 토큰화\r\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\r\n",
        "    return word_tokens\r\n",
        "\r\n",
        "#여러 문장들에 대해 문장별 단어 토큰화 수행. \r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(type(word_tokens),len(word_tokens))\r\n",
        "print(word_tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fBh6HiIFy0u"
      },
      "source": [
        "## Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH6k3l97EE4b",
        "outputId": "ac2d2d43-dec2-4805-853f-8428391053c6"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PHFkFyZEE7M",
        "outputId": "41a7f01a-3839-43f4-8e8e-4529ad0c73db"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\r\n",
        "\r\n",
        "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8rJi7o6EE90",
        "outputId": "558f9a00-840a-4d9d-d981-6bea7c22f2e7"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\r\n",
        "\r\n",
        "import nltk\r\n",
        "\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens = []\r\n",
        "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\r\n",
        "for sentence in word_tokens:\r\n",
        "    filtered_words=[]\r\n",
        "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\r\n",
        "    for word in sentence:\r\n",
        "        #소문자로 모두 변환합니다. \r\n",
        "        word = word.lower()\r\n",
        "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\r\n",
        "        if word not in stopwords:\r\n",
        "            filtered_words.append(word)\r\n",
        "    all_tokens.append(filtered_words)\r\n",
        "    \r\n",
        "print(all_tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gdlLqtsKjyM"
      },
      "source": [
        "## Stemming과 Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm9w71QeEFAz",
        "outputId": "9064bc2a-6ed3-4bf5-e74d-877da23ef558"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원현을 찾는 방법\r\n",
        "# Stemmer(LancasterStemmer)\r\n",
        "\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "\r\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\r\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\r\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))\r\n",
        "\r\n",
        "# 문제가 있음"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eamJMd0aEFDe",
        "outputId": "d91fdf8e-78b0-4dda-d058-72b48be95035"
      },
      "source": [
        "#Lemmatizer (WordNetLemmatizer): 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\r\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bBviHC6EFF0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn_UX-V7EFIT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDAP9u7QEFLD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Ea0DuMEFN3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnd_ZAj8EFQl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJykRMCSEFTN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI0ODkeSEFV7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riPvXfIWEFYr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kKNeNv4EFbd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8GqupQ7EFeT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}