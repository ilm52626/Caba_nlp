{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOHw2pOyDUf85+5ZruT9IIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilm52626/Caba_nlp/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-okGjN-D6sTC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg_pIbnq7Xen"
      },
      "source": [
        "## NLP, 텍스트 분석\r\n",
        "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\r\n",
        "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\r\n",
        "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\r\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\r\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\r\n",
        "    - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\r\n",
        "    - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\r\n",
        "    - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\r\n",
        "    - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 \r\n",
        "\r\n",
        "\r\n",
        "### Text 분석 수행 프로세스\r\n",
        "- 텍스트 정규화\r\n",
        "    - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\r\n",
        "- 피처 벡터화 변환\r\n",
        "    - Bag of Words : Count 기반, TF-IDF 기반\r\n",
        "    - Word2Vec\r\n",
        "- ML 모델 수립 및 학습/예측/평가\r\n",
        "\r\n",
        "### 텍스트 전처리 - 텍스트 정규화\r\n",
        "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\r\n",
        "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\r\n",
        "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\r\n",
        "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\r\n",
        "    - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\r\n",
        "    - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsgeF_F3Fmik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TYOPYv9Fmqn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCI2ySnpFnN2"
      },
      "source": [
        "## Text Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FX7Fx2aC97-",
        "outputId": "7c98b5ab-c109-47bb-8272-fe8fa3d84ca6"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt') #nltk는 영어!\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfSNavFS7LBZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXSiYc23Di3T",
        "outputId": "f92381cf-95d3-456c-b454-608ce02cdf2a"
      },
      "source": [
        "# 문장 토큰화(sent tokenize): 마침표, 개행문자(\\n), 정규표현식\r\n",
        "\r\n",
        "from nltk import sent_tokenize\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\r\n",
        "               You can see it out your window or on your television. \\\r\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\r\n",
        "sentences = sent_tokenize(text=text_sample)\r\n",
        "print(type(sentences),len(sentences))\r\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koBMa5XHD_5s",
        "outputId": "9b625e05-c6b5-4087-989d-831a50e2cbcc"
      },
      "source": [
        "# 단어 토큰화(word_tokenize): 공백, comma, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\r\n",
        "words = word_tokenize(sentence)\r\n",
        "print(type(words), len(words))\r\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJa0Uey_EE1z",
        "outputId": "a2d12c72-1f56-4b04-855d-14b4571f91a1"
      },
      "source": [
        "# 한번에 하는 방법\r\n",
        "from nltk import word_tokenize, sent_tokenize\r\n",
        "\r\n",
        "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\r\n",
        "def tokenize_text(text):\r\n",
        "    \r\n",
        "    # 문장별로 분리 토큰\r\n",
        "    sentences = sent_tokenize(text)\r\n",
        "    # 분리된 문장별 단어 토큰화\r\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\r\n",
        "    return word_tokens\r\n",
        "\r\n",
        "#여러 문장들에 대해 문장별 단어 토큰화 수행. \r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(type(word_tokens),len(word_tokens))\r\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fBh6HiIFy0u"
      },
      "source": [
        "## Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH6k3l97EE4b",
        "outputId": "ac2d2d43-dec2-4805-853f-8428391053c6"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PHFkFyZEE7M",
        "outputId": "41a7f01a-3839-43f4-8e8e-4529ad0c73db"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\r\n",
        "\r\n",
        "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8rJi7o6EE90",
        "outputId": "558f9a00-840a-4d9d-d981-6bea7c22f2e7"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\r\n",
        "\r\n",
        "import nltk\r\n",
        "\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens = []\r\n",
        "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\r\n",
        "for sentence in word_tokens:\r\n",
        "    filtered_words=[]\r\n",
        "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\r\n",
        "    for word in sentence:\r\n",
        "        #소문자로 모두 변환합니다. \r\n",
        "        word = word.lower()\r\n",
        "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\r\n",
        "        if word not in stopwords:\r\n",
        "            filtered_words.append(word)\r\n",
        "    all_tokens.append(filtered_words)\r\n",
        "    \r\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gdlLqtsKjyM"
      },
      "source": [
        "## Stemming과 Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm9w71QeEFAz",
        "outputId": "9064bc2a-6ed3-4bf5-e74d-877da23ef558"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원현을 찾는 방법\r\n",
        "# Stemmer(LancasterStemmer)\r\n",
        "\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "\r\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\r\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\r\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))\r\n",
        "\r\n",
        "# 문제가 있음"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eamJMd0aEFDe",
        "outputId": "d91fdf8e-78b0-4dda-d058-72b48be95035"
      },
      "source": [
        "#Lemmatizer (WordNetLemmatizer): 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\r\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_rxPlpUlcDw"
      },
      "source": [
        "급 GPU 한번 써보기\r\n",
        "## GPU VS CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKmeG-Q2lb2x",
        "outputId": "c95bc438-c318-42dc-ea89-6a588672a977"
      },
      "source": [
        "import numpy as np\r\n",
        "num_samples = 100\r\n",
        "height = 71\r\n",
        "width = 71\r\n",
        "num_classes = 100\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.applications import Xception\r\n",
        "import datetime\r\n",
        "start = datetime.datetime.now()\r\n",
        "\r\n",
        "model = Xception(weights = None,\r\n",
        "                 input_shape = (height, width, 3),\r\n",
        "                 classes = num_classes)\r\n",
        "\r\n",
        "model.compile(loss= 'categorical_crossentropy',\r\n",
        "              optimizer = 'rmsprop')\r\n",
        "x = np.random.random((num_samples, height, width, 3))\r\n",
        "y = np.random.random((num_samples, num_classes))\r\n",
        "\r\n",
        "model.fit(x,y, epochs = 3, batch_size=16)\r\n",
        "model.save('my_model.h5')\r\n",
        "end = datetime.datetime.now()\r\n",
        "time_delta = end - start"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 8s 34ms/step - loss: 235.5010\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 245.7759\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 247.4535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bBviHC6EFF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1ab5fe-31e5-4e39-bfe9-7b381bcb76f8"
      },
      "source": [
        "print('걸린시간 : {}초'.format(time_delta.seconds))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "걸린시간 : 9초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn_UX-V7EFIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137fd303-3679-4c27-e13e-cec7138f1a2a"
      },
      "source": [
        "import numpy as np\r\n",
        "num_samples = 100\r\n",
        "height = 71\r\n",
        "width = 71\r\n",
        "num_classes = 100\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.applications import Xception\r\n",
        "import datetime\r\n",
        "start = datetime.datetime.now()\r\n",
        "\r\n",
        "with tf.device('/cpu:0'): #cpu로 돌리기\r\n",
        "  model = Xception(weights = None,\r\n",
        "                  input_shape = (height, width, 3),\r\n",
        "                  classes = num_classes)\r\n",
        "\r\n",
        "  model.compile(loss= 'categorical_crossentropy',\r\n",
        "                optimizer = 'rmsprop')\r\n",
        "  x = np.random.random((num_samples, height, width, 3))\r\n",
        "  y = np.random.random((num_samples, num_classes))\r\n",
        "\r\n",
        "  model.fit(x,y, epochs = 3, batch_size=16)\r\n",
        "  model.save('my_model.h5')\r\n",
        "end = datetime.datetime.now()\r\n",
        "time_delta = end - start"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 16s 1s/step - loss: 235.8668\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 8s 1s/step - loss: 245.0394\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 8s 1s/step - loss: 247.2511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDAP9u7QEFLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51971b3-b3b9-4246-fc0a-20af63f6dc5c"
      },
      "source": [
        "print('걸린시간 : {}초'.format(time_delta.seconds))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "걸린시간 : 33초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxjL_4DWoPLp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Ea0DuMEFN3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnd_ZAj8EFQl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJykRMCSEFTN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI0ODkeSEFV7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riPvXfIWEFYr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kKNeNv4EFbd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8GqupQ7EFeT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}